{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 诗歌生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "start_token = 'bos'\n",
    "end_token = 'eos'\n",
    "\n",
    "def process_dataset(fileName):\n",
    "    examples = []\n",
    "    with open(fileName, 'r', encoding='utf-8') as fd:  # 指定编码为utf-8\n",
    "        for line in fd:\n",
    "            outs = line.strip().split(':')\n",
    "            content = ''.join(outs[1:])\n",
    "            ins = [start_token] + list(content) + [end_token] \n",
    "            if len(ins) > 200:\n",
    "                continue\n",
    "            examples.append(ins)\n",
    "            \n",
    "    counter = collections.Counter()\n",
    "    for e in examples:\n",
    "        for w in e:\n",
    "            counter[w]+=1\n",
    "    \n",
    "    sorted_counter = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*sorted_counter)\n",
    "    words = ('PAD', 'UNK') + words[:len(words)]\n",
    "    word2id = dict(zip(words, range(len(words))))\n",
    "    id2word = {word2id[k]:k for k in word2id}\n",
    "    \n",
    "    indexed_examples = [[word2id[w] for w in poem]\n",
    "                        for poem in examples]\n",
    "    seqlen = [len(e) for e in indexed_examples]\n",
    "    \n",
    "    instances = list(zip(indexed_examples, seqlen))\n",
    "    \n",
    "    return instances, word2id, id2word\n",
    "\n",
    "def poem_dataset():\n",
    "    instances, word2id, id2word = process_dataset('./poems.txt')\n",
    "    ds = tf.data.Dataset.from_generator(lambda: [ins for ins in instances], \n",
    "                                            (tf.int64, tf.int64), \n",
    "                                            (tf.TensorShape([None]),tf.TensorShape([])))\n",
    "    ds = ds.shuffle(buffer_size=10240)\n",
    "    ds = ds.padded_batch(100, padded_shapes=(tf.TensorShape([None]),tf.TensorShape([])))\n",
    "    ds = ds.map(lambda x, seqlen: (x[:, :-1], x[:, 1:], seqlen-1))\n",
    "    return ds, word2id, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型代码， 完成建模代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNNModel(keras.Model):\n",
    "    def __init__(self, w2id):\n",
    "        super(myRNNModel, self).__init__()\n",
    "        self.v_sz = len(w2id)\n",
    "        self.embed_layer = tf.keras.layers.Embedding(self.v_sz, 64)# self.embed_layer = tf.keras.layers.Embedding(self.v_sz, 64,  batch_input_shape=[None, None])\n",
    "        \n",
    "        self.rnncell = tf.keras.layers.SimpleRNNCell(128)\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnncell, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(self.v_sz)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inp_ids):\n",
    "        '''\n",
    "        此处完成建模过程，可以参考Learn2Carry\n",
    "        ''' \n",
    "        # 将输入序列通过Embedding层\n",
    "        embed_inp = self.embed_layer(inp_ids)  # shape(batch_size, seq_len, emb_sz)\n",
    "        \n",
    "        # 通过RNN层\n",
    "        rnn_output = self.rnn_layer(embed_inp)  # shape(batch_size, seq_len, h_sz)\n",
    "        \n",
    "        # 通过Dense层得到logits\n",
    "        logits = self.dense(rnn_output)  # shape(batch_size, seq_len, v_sz)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @tf.function\n",
    "    def get_next_token(self, x, state):\n",
    "        '''\n",
    "        shape(x) = [b_sz,] \n",
    "        '''\n",
    "        inp_emb = self.embed_layer(x)  # shape(b_sz, emb_sz)\n",
    "        h, state = self.rnncell.call(inp_emb, state)  # shape(b_sz, h_sz)\n",
    "        logits = self.dense(h)  # shape(b_sz, v_sz)\n",
    "        out = tf.argmax(logits, axis=-1)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个计算sequence loss的辅助函数，只需了解用途。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkMask(input_tensor, maxLen):\n",
    "    shape_of_input = tf.shape(input_tensor)\n",
    "    shape_of_output = tf.concat(axis=0, values=[shape_of_input, [maxLen]])\n",
    "\n",
    "    oneDtensor = tf.reshape(input_tensor, shape=(-1,))\n",
    "    flat_mask = tf.sequence_mask(oneDtensor, maxlen=maxLen)\n",
    "    return tf.reshape(flat_mask, shape_of_output)\n",
    "\n",
    "\n",
    "def reduce_avg(reduce_target, lengths, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reduce_target : shape(d_0, d_1,..,d_dim, .., d_k)\n",
    "        lengths : shape(d0, .., d_(dim-1))\n",
    "        dim : which dimension to average, should be a python number\n",
    "    \"\"\"\n",
    "    shape_of_lengths = lengths.get_shape()\n",
    "    shape_of_target = reduce_target.get_shape()\n",
    "    if len(shape_of_lengths) != dim:\n",
    "        raise ValueError(('Second input tensor should be rank %d, ' +\n",
    "                         'while it got rank %d') % (dim, len(shape_of_lengths)))\n",
    "    if len(shape_of_target) < dim+1 :\n",
    "        raise ValueError(('First input tensor should be at least rank %d, ' +\n",
    "                         'while it got rank %d') % (dim+1, len(shape_of_target)))\n",
    "\n",
    "    rank_diff = len(shape_of_target) - len(shape_of_lengths) - 1\n",
    "    mxlen = tf.shape(reduce_target)[dim]\n",
    "    mask = mkMask(lengths, mxlen)\n",
    "    if rank_diff!=0:\n",
    "        len_shape = tf.concat(axis=0, values=[tf.shape(lengths), [1]*rank_diff])\n",
    "        mask_shape = tf.concat(axis=0, values=[tf.shape(mask), [1]*rank_diff])\n",
    "    else:\n",
    "        len_shape = tf.shape(lengths)\n",
    "        mask_shape = tf.shape(mask)\n",
    "    lengths_reshape = tf.reshape(lengths, shape=len_shape)\n",
    "    mask = tf.reshape(mask, shape=mask_shape)\n",
    "\n",
    "    mask_target = reduce_target * tf.cast(mask, dtype=reduce_target.dtype)\n",
    "\n",
    "    red_sum = tf.reduce_sum(mask_target, axis=[dim], keepdims=False)\n",
    "    red_avg = red_sum / (tf.cast(lengths_reshape, dtype=tf.float32) + 1e-30)\n",
    "    return red_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义loss函数，定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels, seqlen):\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "    losses = reduce_avg(losses, seqlen, dim=1)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y, seqlen):\n",
    "    '''\n",
    "    完成一步优化过程，可以参考之前做过的模型\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 前向传播\n",
    "        logits = model(x)  # shape(batch_size, seq_len, v_sz)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = compute_loss(logits, y, seqlen)\n",
    "    \n",
    "    # 计算梯度\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # 更新模型参数\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train(epoch, model, optimizer, ds):\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for step, (x, y, seqlen) in enumerate(ds):\n",
    "        loss = train_one_step(model, optimizer, x, y, seqlen)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print('epoch', epoch, ': loss', loss.numpy())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练优化过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\k1xwc\\AppData\\Local\\Temp\\ipykernel_32600\\2210628749.py\", line 24, in train_one_step  *\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    File \"d:\\Programs\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 383, in apply_gradients  **\n        self.apply(grads, trainable_variables)\n    File \"d:\\Programs\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 436, in apply\n        grads, trainable_variables = self._filter_empty_gradients(\n    File \"d:\\Programs\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 772, in _filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable.\")\n\n    ValueError: No gradients provided for any variable.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model = myRNNModel(word2id)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epoch, model, optimizer, ds)\u001b[39m\n\u001b[32m     30\u001b[39m accuracy = \u001b[32m0.0\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, (x, y, seqlen) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ds):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     loss = \u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[32m500\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m, epoch, \u001b[33m'\u001b[39m\u001b[33m: loss\u001b[39m\u001b[33m'\u001b[39m, loss.numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\python\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\__autograph_generated_filen1enidvi.py:17\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_one_step\u001b[39m\u001b[34m(model, optimizer, x, y, seqlen)\u001b[39m\n\u001b[32m     15\u001b[39m     loss = ag__.converted_call(ag__.ld(compute_loss), (ag__.ld(logits), ag__.ld(y), ag__.ld(seqlen)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     16\u001b[39m grads = ag__.converted_call(ag__.ld(tape).gradient, (ag__.ld(loss), ag__.ld(model).trainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     19\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:383\u001b[39m, in \u001b[36mBaseOptimizer.apply_gradients\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[32m    382\u001b[39m     grads, trainable_variables = \u001b[38;5;28mzip\u001b[39m(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterations\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:436\u001b[39m, in \u001b[36mBaseOptimizer.apply\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    429\u001b[39m grads, trainable_variables = (\n\u001b[32m    430\u001b[39m     \u001b[38;5;28mself\u001b[39m._overwrite_variables_directly_with_gradients(\n\u001b[32m    431\u001b[39m         grads, trainable_variables\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m )\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# Filter empty gradients.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m grads, trainable_variables = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(grads)) == \u001b[32m0\u001b[39m:\n\u001b[32m    440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:772\u001b[39m, in \u001b[36mBaseOptimizer._filter_empty_gradients\u001b[39m\u001b[34m(self, grads, vars)\u001b[39m\n\u001b[32m    769\u001b[39m             missing_grad_vars.append(v.name)\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered_grads:\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo gradients provided for any variable.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_grad_vars:\n\u001b[32m    774\u001b[39m     warnings.warn(\n\u001b[32m    775\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGradients do not exist for variables \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    776\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(missing_grad_vars))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when minimizing the loss.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    777\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m If using `model.compile()`, did you forget to provide a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    778\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`loss` argument?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    779\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: in user code:\n\n    File \"C:\\Users\\k1xwc\\AppData\\Local\\Temp\\ipykernel_32600\\2210628749.py\", line 24, in train_one_step  *\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    File \"d:\\Programs\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 383, in apply_gradients  **\n        self.apply(grads, trainable_variables)\n    File \"d:\\Programs\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 436, in apply\n        grads, trainable_variables = self._filter_empty_gradients(\n    File \"d:\\Programs\\python\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 772, in _filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable.\")\n\n    ValueError: No gradients provided for any variable.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(0.0005)\n",
    "train_ds, word2id, id2word = poem_dataset()\n",
    "model = myRNNModel(word2id)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = train(epoch, model, optimizer, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentence():\n",
    "    state = [tf.random.normal(shape=(1, 128), stddev=0.5), tf.random.normal(shape=(1, 128), stddev=0.5)]\n",
    "    cur_token = tf.constant([word2id['bos']], dtype=tf.int32)\n",
    "    collect = []\n",
    "    for _ in range(50):\n",
    "        cur_token, state = model.get_next_token(cur_token, state)\n",
    "        collect.append(cur_token.numpy()[0])\n",
    "    return [id2word[t] for t in collect]\n",
    "print(''.join(gen_sentence()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
